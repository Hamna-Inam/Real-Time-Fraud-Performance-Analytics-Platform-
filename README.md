

# Real-Time Fraud & Performance Analytics Platform

A **production-grade, real-time Big Data analytics pipeline** for digital payments, built as part of the **Big Data Analytics course group project**.
The system simulates live transaction streams, performs real-time KPI computation, enforces storage governance, and visualizes insights via a live dashboard.

---

## ğŸ“Œ Problem Domain

**Domain:** FinTech / Digital Payments

Modern payment platforms process millions of transactions per day. Batch analytics are insufficient for:

* **Real-time fraud detection**
* **Operational health monitoring**
* **Revenue velocity tracking**

This project implements a **streaming architecture** to deliver immediate insights with low latency.

---

## ğŸ—ï¸ System Architecture (High Level)

```
<img width="404" height="660" alt="image" src="https://github.com/user-attachments/assets/dd67f40d-465e-4b0d-a704-42d2d882d00a" />

```

---

## ğŸ”§ Technology Stack

| Component              | Technology              |
| ---------------------- | ----------------------- |
| Orchestration          | Apache Airflow          |
| Containerization       | Docker & Docker Compose |
| Streaming / Processing | Apache Spark (PySpark)  |
| NoSQL Database         | MongoDB                 |
| Distributed Storage    | Hadoop HDFS             |
| Caching                | Redis                   |
| Visualization          | Streamlit               |
| Data Generation        | SDV (CTGAN)             |

---

## ğŸ“‚ Repository Structure

```
BDA-PROJECT/
â”‚
â”œâ”€â”€ dags/                     # Airflow DAGs
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ archive/              # HDFS archived data (Parquet)
â”‚
â”œâ”€â”€ jars/                     # Spark / connector JARs
â”œâ”€â”€ logs/                     # Airflow & Spark logs
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ai_generator.py       # AI-based streaming data generator
â”‚   â”œâ”€â”€ archive_job.py        # Spark job: MongoDB â†’ HDFS
â”‚   â”œâ”€â”€ kpi_job.py            # Spark job: KPI computation
â”‚   â””â”€â”€ setup_dimensions.py  # Dimension table setup
â”‚
â”œâ”€â”€ streamlit/
â”‚   â””â”€â”€ app.py                # Live BI dashboard
â”‚
â”œâ”€â”€ Dockerfile                # Service container definitions
â”œâ”€â”€ docker-compose.yml        # Full system orchestration
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸ”„ Data Pipeline Overview

### 1ï¸âƒ£ Data Generation

* **AI-based streaming generator** using **CTGAN (SDV)**
* Learns realistic fraud and transaction patterns
* Streams one transaction per second into MongoDB

ğŸ“„ `scripts/ai_generator.py`

---

### 2ï¸âƒ£ Operational Storage (Hot)

* MongoDB stores high-velocity incoming data
* Acts as the **fact table** in a star schema

---

### 3ï¸âƒ£ KPI Processing (Spark)

Spark performs:

* Data cleansing & validation
* Fact â†’ Dimension joins
* Aggregations for KPIs

**Key KPIs:**

* Total Transaction Volume
* Average Ticket Size
* Fraud Rate (%)
* Processing Latency
* Transactions Per Minute
* Revenue by Category

ğŸ“„ `scripts/kpi_job.py`

---

### 4ï¸âƒ£ Caching Layer

* Aggregated KPIs pushed to **Redis**
* Enables **sub-second dashboard refresh**
* Prevents repeated database scans

---

### 5ï¸âƒ£ Archiving & Data Governance

* MongoDB size monitored continuously
* **300MB hot-storage threshold**
* Older data archived to **HDFS (Parquet format)**
* Metadata logged for traceability

ğŸ“„ `scripts/archive_job.py`

---

## â±ï¸ Airflow Orchestration

Two production DAGs orchestrate the system:

### ğŸ”¹ `kpi_refresh_dag`

* Runs periodically
* Triggers Spark KPI job
* Updates Redis cache

### ğŸ”¹ `mongo_to_hdfs_archiving`

* Monitors MongoDB size
* Archives old data to HDFS
* Cleans hot storage

ğŸ“ `dags/`

---

## ğŸ“Š Live Dashboard (Streamlit)

Features:

* Auto-refresh every 30 seconds
* Live transaction metrics
* Fraud spikes visualization
* Revenue & category analytics

ğŸ“„ `streamlit/app.py`

---

## ğŸš€ How to Run

```bash
# Start the full system
docker-compose up -d

# Access Airflow
http://localhost:8080

# Access Streamlit Dashboard
http://localhost:8501
```

---

## âœ… Project Outcome

* Fully containerized, scalable architecture
* Real-time analytics with low latency
* Automated orchestration & governance
* Production-ready Big Data pipeline

---

## ğŸ“œ License

This project is developed for academic purposes as part of the **Big Data Analytics course**.

